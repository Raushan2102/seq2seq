{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import editdistance\n",
    "from transer import Dataset\n",
    "from transer import Trainer\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('train.csv')\n",
    "words_vocab = dataset.words_vocab\n",
    "trans_vocab = dataset.trans_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, pad_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        self.gru       = nn.GRU(emb_size, hidden_size, batch_first = True)\n",
    "    \n",
    "    def forward(self, source, source_lens=None):\n",
    "        embedded = self.embedding(source)\n",
    "        \n",
    "        if source_lens is not None:\n",
    "            embedded = pack(embedded, source_lens, batch_first = True)\n",
    "  \n",
    "        outs, hidden = self.gru(embedded)\n",
    "    \n",
    "        if source_lens is not None:\n",
    "            outs,_ = unpack(outs, batch_first = True)\n",
    "        \n",
    "        return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 14, 64]) torch.Size([1, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_words, batch_trans_in, batch_trans_out, words_lens, trans_lens = dataset.get_batch(32, sort = True)\n",
    "encoder = Encoder(len(words_vocab), 32, 64, pad_idx = words_vocab.pad_idx)\n",
    "outputs, hidden = encoder(batch_words) \n",
    "print(outputs.size(), hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
